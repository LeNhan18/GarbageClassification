# train_model2B.py
# Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i r√°c kh√¥ng t√°i ch·∫ø

import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, regularizers, optimizers
from tensorflow.keras.applications import EfficientNetB2  # N√¢ng c·∫•p l√™n B2
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
import matplotlib.pyplot as plt  # S·ª≠a l·ªói import
import tensorflow as tf

# --- C·∫•u h√¨nh GPU ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô ---
try:
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        tf.config.experimental.set_virtual_device_configuration(
            gpus[0],
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]
        )
        print("ƒê√£ c·∫•u h√¨nh GPU th√†nh c√¥ng")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y GPU")
except Exception as e:
    print(f"‚ùå L·ªói c·∫•u h√¨nh GPU: {e}")

# --- C·∫•u h√¨nh ---
data_dir = 'Z:\\GarbageClassification\\data\\non_recyclable'
img_size = (240, 240)  # TƒÉng k√≠ch th∆∞·ªõc ·∫£nh m·ªôt ch√∫t
batch_size = 16  # Gi·∫£m batch size ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
epochs = 70  # TƒÉng epochs
input_shape = (240, 240, 3)

# --- Data Augmentation m·∫°nh h∆°n ---
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    validation_split=0.2,
    rotation_range=30,  # TƒÉng rotation
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,  # Th√™m l·∫≠t d·ªçc
    fill_mode='nearest',
    brightness_range=[0.7, 1.3]
)

val_datagen = ImageDataGenerator(
    rescale=1. / 255,
    validation_split=0.2
)

# T·∫°o generators
train_generator = train_datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_generator = val_datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# --- T·∫°o th∆∞ m·ª•c l∆∞u m√¥ h√¨nh v√† logs ---
models_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model')
logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
os.makedirs(models_dir, exist_ok=True)
os.makedirs(logs_dir, exist_ok=True)

# --- X√¢y d·ª±ng m√¥ h√¨nh v·ªõi EfficientNetB2 ---
base_model = EfficientNetB2(  # N√¢ng c·∫•p t·ª´ B0 l√™n B2
    weights='imagenet',
    include_top=False,
    input_shape=input_shape
)

# Fine-tune nhi·ªÅu layer h∆°n
fine_tune_at = 100  # Gi·∫£m ƒë·ªÉ fine-tune nhi·ªÅu layer h∆°n
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.BatchNormalization(),

    # First Dense Block
    layers.Dense(1024, kernel_regularizer=regularizers.l2(0.0005)),  # TƒÉng neurons v√† gi·∫£m regularization
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(0.4),

    # Second Dense Block - th√™m m·ªôt l·ªõp hidden
    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0005)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dropout(0.3),

    # Output Layer
    layers.Dense(len(train_generator.class_indices), activation='softmax')
])

# --- T·ªëi ∆∞u qu√° tr√¨nh hu·∫•n luy·ªán ---
# S·ª≠a l·ªói LearningRateSchedule b·∫±ng c√°ch s·ª≠ d·ª•ng learning rate c·ªë ƒë·ªãnh
initial_learning_rate = 1e-4

# Thay th·∫ø lr_schedule b·∫±ng gi√° tr·ªã c·ªë ƒë·ªãnh
optimizer = optimizers.Adam(
    learning_rate=initial_learning_rate,  # S·ª≠ d·ª•ng gi√° tr·ªã c·ªë ƒë·ªãnh thay v√¨ schedule
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07
)

model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy')]
)

# --- Callbacks ---
callbacks = [
    ModelCheckpoint(
        os.path.join(models_dir, 'model2B_best.keras'),
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=15,  # TƒÉng patience
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(  # S·ª≠ d·ª•ng ReduceLROnPlateau thay v√¨ schedule
        monitor='val_loss',
        factor=0.5,
        patience=7,
        min_lr=1e-7,
        verbose=1
    ),
    CSVLogger(os.path.join(logs_dir, 'model2B_training.csv'))
]

# --- Hu·∫•n luy·ªán m√¥ h√¨nh ---
print("\nB·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Model 2B (Ph√¢n lo·∫°i r√°c kh√¥ng t√°i ch·∫ø)...")
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=epochs,
    callbacks=callbacks,
    verbose=1
)

# --- L∆∞u m√¥ h√¨nh v√† ƒë√°nh gi√° ---
model.save(os.path.join(models_dir, 'model2B_final.keras'))
print("‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh th√†nh c√¥ng!")

# ƒê√°nh gi√° m√¥ h√¨nh
val_metrics = model.evaluate(val_generator, verbose=1)
metrics_names = ['loss', 'accuracy', 'top_2_accuracy']
print("\nK·∫øt qu·∫£ ƒë√°nh gi√° Model 2B:")
for name, value in zip(metrics_names, val_metrics):
    print(f"{name}: {value:.4f}")

# V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model 2B Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model 2B Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig(os.path.join(logs_dir, 'model2B_training_history.png'))
plt.close()

print("‚úÖ Ho√†n th√†nh qu√° tr√¨nh hu·∫•n luy·ªán Model 2B.")
# --- T√≠nh class_weight ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu ---
from sklearn.utils import class_weight
import numpy as np

# T√≠nh class weights
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weights_dict = dict(enumerate(class_weights))

# ‚úÖ In ra ƒë·ªÉ ki·ªÉm tra
print("\nClass weights:")
for label, weight in class_weights_dict.items():
    class_name = list(train_generator.class_indices.keys())[label]
    print(f"{class_name}: {weight:.2f}")

# --- Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh v·ªõi class_weight ---
print("\n‚ö†Ô∏è Ti·∫øn h√†nh HU·∫§N LUY·ªÜN L·∫†I Model 2B v·ªõi class_weight...")
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=epochs,
    callbacks=callbacks,
    class_weight=class_weights_dict,
    verbose=1
)

# --- ƒê√°nh gi√° chi ti·∫øt b·∫±ng classification_report ---
from sklearn.metrics import classification_report

# D·ª± ƒëo√°n tr√™n validation set
val_generator.reset()
pred_probs = model.predict(val_generator)
pred_classes = np.argmax(pred_probs, axis=1)
true_classes = val_generator.classes
class_labels = list(val_generator.class_indices.keys())

# In b√°o c√°o ph√¢n lo·∫°i
report = classification_report(true_classes, pred_classes, target_names=class_labels)
print("\nüìã B√°o c√°o ph√¢n lo·∫°i Model 2B:")
print(report)

# (Kh√¥ng c·∫ßn v·∫Ω l·∫°i bi·ªÉu ƒë·ªì n·∫øu kh√¥ng thay ƒë·ªïi ki·∫øn tr√∫c/model)

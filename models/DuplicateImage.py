import os
import hashlib
from PIL import Image
import imagehash
import shutil
from datetime import datetime

# --- C·∫•u h√¨nh ---
data_dir = 'Z:\\GarbageClassification\\datas\\non_recyclable'  # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu
output_log_dir = 'Z:\\GarbageClassification\\logs'  # Th∆∞ m·ª•c l∆∞u log
backup_dir = 'Z:\\GarbageClassification\\backup\\trash'  # Th∆∞ m·ª•c l∆∞u b·∫£n sao tr∆∞·ªõc khi x√≥a
similarity_threshold = 10  # Ng∆∞·ª°ng cho perceptual hash (tƒÉng l√™n 10)


def get_file_hash(file_path):
    try:
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"L·ªói khi t√≠nh hash cho {file_path}: {e}")
        return None
def get_perceptual_hash(file_path):
    try:
        img = Image.open(file_path)
        return imagehash.difference_hash(img)  # S·ª≠ d·ª•ng difference hash thay v√¨ average hash
    except Exception as e:
        print(f"L·ªói khi t√≠nh perceptual hash cho {file_path}: {e}")
        return None
def find_and_remove_duplicates(data_dir, backup_dir, output_log_dir):
    # T·∫°o th∆∞ m·ª•c backup v√† log
    os.makedirs(backup_dir, exist_ok=True)
    os.makedirs(output_log_dir, exist_ok=True)

    # T·∫°o file log v·ªõi timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(output_log_dir, f'duplicate_removal_log_{timestamp}.txt')

    # Thu th·∫≠p t·∫•t c·∫£ file ·∫£nh
    image_files = []
    for root, _, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_files.append(os.path.join(root, file))

    print(f"T√¨m th·∫•y {len(image_files)} ·∫£nh trong th∆∞ m·ª•c.")
#Hash anh timf kiem so sanh anh khac nhau
    # L∆∞u tr·ªØ hash v√† ƒë∆∞·ªùng d·∫´n
    hash_dict = {}
    duplicates = []

    for file_path in image_files:
        img_hash = get_file_hash(file_path)
        if img_hash is None:
            continue

        if img_hash in hash_dict:
            duplicates.append((file_path, hash_dict[img_hash]))
        else:
            hash_dict[img_hash] = file_path

    # Ghi log v√† x·ª≠ l√Ω ·∫£nh tr√πng l·∫∑p
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"Duplicate Removal Log - {timestamp}\n")
        f.write(f"Total images scanned: {len(image_files)}\n")
        f.write(f"Duplicates found: {len(duplicates)}\n\n")

        if duplicates:
            print(f"üóëT√¨m th·∫•y {len(duplicates)} ·∫£nh tr√πng l·∫∑p. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
            for dup, orig in duplicates:
                # Sao l∆∞u ·∫£nh tr√πng l·∫∑p v√†o backup_dir
                backup_path = os.path.join(backup_dir, os.path.relpath(dup, data_dir))
                os.makedirs(os.path.dirname(backup_path), exist_ok=True)
                shutil.copy(dup, backup_path)

                # X√≥a ·∫£nh tr√πng l·∫∑p
                try:
                    os.remove(dup)
                    log_message = f"Removed: {dup} (Original: {orig})\n"
                    print(log_message.strip())
                except Exception as e:
                    log_message = f"Error removing {dup}: {e}\n"
                    print(f"{log_message.strip()}")

                f.write(log_message)
        else:
            print("Kh√¥ng t√¨m th·∫•y ·∫£nh tr√πng l·∫∑p.")
            f.write("No duplicates found.\n")

    return len(duplicates)


# --- Ki·ªÉm tra ·∫£nh t∆∞∆°ng t·ª± (kh√¥ng x√≥a, ch·ªâ li·ªát k√™) ---
def find_similar_images(data_dir, output_log_dir, threshold=10):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(output_log_dir, f'similar_images_log_{timestamp}.txt')

    image_files = []
    for root, _, files in os.walk(data_dir):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_files.append(os.path.join(root, file))

    print(f"üì∏ T√¨m th·∫•y {len(image_files)} ·∫£nh ƒë·ªÉ ki·ªÉm tra t∆∞∆°ng t·ª±.")

    hash_dict = {}
    similar_pairs = []

    for file_path in image_files:
        img_hash = get_perceptual_hash(file_path)
        if img_hash is None:
            continue

        for existing_hash, existing_path in hash_dict.items():
            # T√≠nh kho·∫£ng c√°ch Hamming
            hamming_distance = img_hash - existing_hash  # Difference hash h·ªó tr·ª£ tr·ª´ tr·ª±c ti·∫øp
            if hamming_distance <= threshold:
                similar_pairs.append((file_path, existing_path, hamming_distance))

        hash_dict[img_hash] = file_path

    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"Similar Images Log - {timestamp}\n")
        f.write(f"Total images scanned: {len(image_files)}\n")
        f.write(f"Similarity threshold: {threshold}\n")
        f.write(f"Similar pairs found: {len(similar_pairs)}\n\n")

        if similar_pairs:
            for file1, file2, distance in similar_pairs:
                log_message = f"Similar: {file1} and {file2} (Hamming distance: {distance})\n"
                f.write(log_message)
        else:
            f.write("No similar images found.\n")

    print(f" T√¨m th·∫•y {len(similar_pairs)} c·∫∑p ·∫£nh t∆∞∆°ng t·ª±. Ki·ªÉm tra log t·∫°i {log_file}.")
    return similar_pairs


# --- Ch·∫°y ch∆∞∆°ng tr√¨nh ---
if __name__ == "__main__":
    print(" B·∫Øt ƒë·∫ßu ki·ªÉm tra v√† lo·∫°i b·ªè ·∫£nh tr√πng l·∫∑p...")

    # B∆∞·ªõc 1: X√≥a ·∫£nh gi·ªëng h·ªát (MD5 hash)
    num_duplicates = find_and_remove_duplicates(data_dir, backup_dir, output_log_dir)

    # B∆∞·ªõc 2: Ki·ªÉm tra ·∫£nh t∆∞∆°ng t·ª± (kh√¥ng x√≥a)
    print("\n Ki·ªÉm tra ·∫£nh t∆∞∆°ng t·ª±...")
    similar_pairs = find_similar_images(data_dir, output_log_dir, similarity_threshold)

    print(f"\n Ho√†n th√†nh! T√¨m th·∫•y v√† x√≥a {num_duplicates} ·∫£nh tr√πng l·∫∑p.")
    print(f" Log ƒë∆∞·ª£c l∆∞u t·∫°i: {output_log_dir}")
    print(f" B·∫£n sao ·∫£nh tr√πng l·∫∑p ƒë∆∞·ª£c l∆∞u t·∫°i: {backup_dir}")